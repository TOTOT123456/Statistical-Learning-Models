{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8ce1517f-7258-406d-9139-9adadb1a1570",
      "metadata": {
        "id": "8ce1517f-7258-406d-9139-9adadb1a1570"
      },
      "source": [
        "<img src=\"https://www.luxonis.com/logo.svg\" width=\"400\">\n",
        "\n",
        "# DataDreamer Tutorial: Generating a dataset for object detection, training a model, and deploying it to the OAK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "622efdfa",
      "metadata": {
        "id": "622efdfa"
      },
      "source": [
        "Install the required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5_2ivH03etO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5_2ivH03etO",
        "outputId": "28b3a940-9f88-4f0c-cf4a-078fe5f390dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datadreamer\n",
            "  Downloading datadreamer-0.2.1-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting torch<=2.5.1,>=2.0.0 (from datadreamer)\n",
            "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: torchvision>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.45.2 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (4.54.1)\n",
            "Requirement already satisfied: diffusers>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (0.34.0)\n",
            "Collecting compel>=2.0.0 (from datadreamer)\n",
            "  Downloading compel-2.1.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tqdm>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (4.67.1)\n",
            "Requirement already satisfied: Pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (11.3.0)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (4.12.0.88)\n",
            "Requirement already satisfied: accelerate>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (1.9.0)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (1.16.1)\n",
            "Collecting bitsandbytes>=0.42.0 (from datadreamer)\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: nltk>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (3.9.1)\n",
            "Collecting luxonis-ml>=0.6.1 (from luxonis-ml[all]>=0.6.1->datadreamer)\n",
            "  Downloading luxonis_ml-0.7.3-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: python-box>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (7.3.2)\n",
            "Requirement already satisfied: gcsfs>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (2025.3.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (0.2.0)\n",
            "Collecting optimum-quanto>=0.2.6 (from datadreamer)\n",
            "  Downloading optimum_quanto-0.2.7-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: huggingface_hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (0.34.3)\n",
            "Collecting loguru>=0.7.0 (from datadreamer)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pycocotools>=2.0.7 in /usr/local/lib/python3.11/dist-packages (from datadreamer) (2.0.10)\n",
            "Collecting sam2>=1.1.0 (from datadreamer)\n",
            "  Downloading sam2-1.1.0.tar.gz (152 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install datadreamer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3704c07",
      "metadata": {
        "id": "c3704c07"
      },
      "source": [
        "## üóÉÔ∏è Generate a dataset with your own classes (might take some time to download all models)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M4v-QieP4tXL",
      "metadata": {
        "id": "M4v-QieP4tXL"
      },
      "source": [
        "Make sure you are using the GPU runtime type (in Google Colab).\n",
        "\n",
        "~8 min to generate 100 images\n",
        "\n",
        "~2 min to annotate them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab1e2f9",
      "metadata": {
        "id": "6ab1e2f9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!datadreamer --save_dir generated_dataset \\\n",
        "             --class_names robot tractor horse car person bear \\\n",
        "             --prompts_number 100 \\\n",
        "             --disable_lm_filter \\\n",
        "             --prompt_generator simple \\\n",
        "             --num_objects_range 2 3 \\\n",
        "             --image_generator sdxl-turbo \\\n",
        "             --use_tta \\\n",
        "             --image_annotator owlv2 \\\n",
        "             --conf_threshold 0.15 \\\n",
        "             --vis_anns \\\n",
        "             --seed 42"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a10755e",
      "metadata": {
        "id": "7a10755e"
      },
      "source": [
        "### Parameters\n",
        "- `--save_dir` (required): Path to the directory for saving generated images and annotations.\n",
        "- `--class_names` (required): Space-separated list of object names for image generation and annotation. Example: `person moon robot`.\n",
        "- `--prompts_number` (optional): Number of prompts to generate for each object. Defaults to `10`.\n",
        "- `--annotate_only` (optional): Only annotate the images without generating new ones, prompt and image generator will be skipped. Defaults to `False`.\n",
        "- `--task`: Choose between `detection`, `classification` and `instance-segmentation`. Default is `detection`.\n",
        "- `--dataset_format`: Format of the dataset. Defaults to `raw`. Supported values: `raw`, `yolo`, `coco`, `voc`, `luxonis-dataset`, `cls-single`.\n",
        "- `--split_ratios`: Split ratios for train, validation, and test sets. Defaults to `[0.8, 0.1, 0.1]`.\n",
        "- `--num_objects_range`: Range of objects in a prompt. Default is 1 to 3.\n",
        "- `--prompt_generator`: Choose between `simple`, `lm` (Mistral-7B), `tiny` (tiny LM), and `qwen2` (Qwen2.5 LM). Default is `qwen2`.\n",
        "- `--image_generator`: Choose image generator, e.g., `sdxl`, `sdxl-turbo`, `sdxl-lightning` or `shuttle-3`. Default is `sdxl-turbo`.\n",
        "- `--image_annotator`: Specify the image annotator, like `owlv2` for object detection or `aimv2` or `clip` for image classification or `owlv2-slimsam` and `owlv2-sam2` for instance segmentation. Default is `owlv2`.\n",
        "- `--conf_threshold`: Confidence threshold for annotation. Default is `0.15`.\n",
        "- `--annotation_iou_threshold`: Intersection over Union (IoU) threshold for annotation. Default is `0.2`.\n",
        "- `--prompt_prefix`: Prefix to add to every image generation prompt. Default is `\"\"`.\n",
        "- `--prompt_suffix`: Suffix to add to every image generation prompt, e.g., for adding details like resolution. Default is `\", hd, 8k, highly detailed\"`.\n",
        "- `--negative_prompt`: Negative prompts to guide the generation away from certain features. Default is `\"cartoon, blue skin, painting, scrispture, golden, illustration, worst quality, low quality, normal quality:2, unrealistic dream, low resolution,  static, sd character, low quality, low resolution, greyscale, monochrome, nose, cropped, lowres, jpeg artifacts, deformed iris, deformed pupils, bad eyes, semi-realistic worst quality, bad lips, deformed mouth, deformed face, deformed fingers, bad anatomy\"`.\n",
        "- `--use_tta`: Toggle test time augmentation for object detection. Default is `False`.\n",
        "- `--synonym_generator`: Enhance class names with synonyms. Default is `none`. Other options are `llm`, `wordnet`.\n",
        "- `--use_image_tester`: Use image tester for image generation. Default is `False`.\n",
        "- `--image_tester_patience`: Patience level for image tester. Default is `1`.\n",
        "- `--lm_quantization`: Quantization to use for Mistral language model. Choose between `none` and `4bit`. Default is `none`.\n",
        "- `--annotator_size`: Size of the annotator model to use. Choose between `base` and `large`. Default is `base`.\n",
        "- `--disable_lm_filter`: Use only a bad word list for profanity filtering. Default is `False`.\n",
        "- `--keep_unlabeled_images`: Whether to keep images without any annotations. Default if `False`.\n",
        "- `--batch_size_prompt`: Batch size for prompt generation. Default is 64.\n",
        "- `--batch_size_annotation`: Batch size for annotation. Default is `1`.\n",
        "- `--batch_size_image`: Batch size for image generation. Default is `1`.\n",
        "- `--raw_mask_format`: Format of segmentations masks when saved in raw dataset format. Default is `rle`.\n",
        "- `--vis_anns`: Whether to save visualizations of annotations. Default is `False`.\n",
        "- `--device`: Choose between `cuda` and `cpu`. Default is `cuda`.\n",
        "- `--seed`: Set a random seed for image and prompt generation. Default is `42`.\n",
        "- `--config`: A path to an optional `.yaml` config file specifying the pipeline's arguments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7add74d9",
      "metadata": {
        "id": "7add74d9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "Image(filename=os.path.join(\"generated_dataset/bboxes_visualization\", \"bbox_0000070.jpg\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64fe2dc9",
      "metadata": {
        "id": "64fe2dc9"
      },
      "source": [
        "## ‚úç Convert the dataset to YOLO format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dd01a6a",
      "metadata": {
        "id": "3dd01a6a"
      },
      "outputs": [],
      "source": [
        "from datadreamer.utils.convert_dataset import convert_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b9bb74d",
      "metadata": {
        "id": "9b9bb74d"
      },
      "outputs": [],
      "source": [
        "convert_dataset(\n",
        "    input_dir=\"generated_dataset\",\n",
        "    output_dir=\"generated_dataset_yolo\",\n",
        "    dataset_format=\"yolo\",\n",
        "    split_ratios=[0.8, 0.1, 0.1],\n",
        "    copy_files=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a167a842",
      "metadata": {
        "id": "a167a842"
      },
      "outputs": [],
      "source": [
        "!ls generated_dataset_yolo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2d660b0",
      "metadata": {
        "id": "d2d660b0"
      },
      "source": [
        "## üèãÔ∏è‚Äç‚ôÇÔ∏è Train your model (YOLOv8 as an example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "982e475e",
      "metadata": {
        "id": "982e475e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184cf0fa",
      "metadata": {
        "id": "184cf0fa"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")  # load a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb4e6754",
      "metadata": {
        "id": "bb4e6754",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "results = model.train(data=\"generated_dataset_yolo/data.yaml\", epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8b05e33",
      "metadata": {
        "id": "d8b05e33"
      },
      "source": [
        "### üß† Show the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b559b1f9",
      "metadata": {
        "id": "b559b1f9"
      },
      "outputs": [],
      "source": [
        "Image(filename=os.path.join(results.save_dir, \"val_batch0_pred.jpg\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec0cb11",
      "metadata": {
        "id": "dec0cb11"
      },
      "outputs": [],
      "source": [
        "metrics = model.val()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "901bd74c",
      "metadata": {
        "id": "901bd74c"
      },
      "source": [
        "## üíæ Weights Download\n",
        "\n",
        "By default, the library saves the weights with the best performance into the `best.pt` file inside the corresponding run folder. We'll rename it to reflect the model better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l7sTTf01i6XP",
      "metadata": {
        "id": "l7sTTf01i6XP"
      },
      "outputs": [],
      "source": [
        "!cp runs/detect/train/weights/best.pt yolov8n_trained_datadreamer.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b0036e6",
      "metadata": {
        "id": "6b0036e6"
      },
      "source": [
        "We'll download the weights to convert them in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ExzeyP2XZsy",
      "metadata": {
        "id": "5ExzeyP2XZsy"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"yolov8n_trained_datadreamer.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f169e60d",
      "metadata": {
        "id": "f169e60d"
      },
      "source": [
        "<a name=\"conversion\"></a>\n",
        "\n",
        "## üóÇÔ∏è Conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93ca103d",
      "metadata": {
        "id": "93ca103d"
      },
      "source": [
        "Now that we have successfully trained the model, we aim to deploy it to the Luxonis device. The model's specific format depends on the Luxonis device series you have. We will show you how to use our [`ModelConverter`](https://github.com/luxonis/modelconverter) to convert the model as simply as possible.\n",
        "\n",
        "We'll start by installing the `ModelConverter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81bb486e",
      "metadata": {
        "id": "81bb486e"
      },
      "outputs": [],
      "source": [
        "%pip install -q modelconv==0.4.0 -U"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93afcb8d",
      "metadata": {
        "id": "93afcb8d"
      },
      "source": [
        "\n",
        "We will use the `ModelConverter` Python API, which leverages our [`HubAI`](https://hub.luxonis.com) platform to perform model conversion in the background. To get started, you'll need to create an account on `HubAI` and obtain your team‚Äôs API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a2524d",
      "metadata": {
        "id": "41a2524d"
      },
      "outputs": [],
      "source": [
        "HUBAI_API_KEY = \"<YOUR_HUBAI_API_KEY>\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c983ee74",
      "metadata": {
        "id": "c983ee74"
      },
      "source": [
        "Model conversion can be done via either the CLI or the Python API ‚Äî here, we'll use the latter. For more information, see the [online usage section](https://github.com/luxonis/modelconverter?tab=readme-ov-file#online-usage) of the documentation.\n",
        "\n",
        "The call below creates a new model card within your team on `HubAI`, uploads the model file and metadata, then performs cloud-side conversion to the selected target platform (e.g., [`RVC2`](https://rvc4.docs.luxonis.com/hardware/platform/rvc/rvc2/), [`RVC4`](https://rvc4.docs.luxonis.com/hardware/platform/rvc/rvc4/)). Once completed, the converted model is automatically downloaded to your device.\n",
        "\n",
        "For HubAI-specific conversion parameters, refer to the [online conversion section](https://github.com/luxonis/modelconverter/tree/e6a3478ba47d8f92d4d60217f2aee0f4f468cb14/modelconverter/hub#online-conversion) of the ModelConverter documentation. Platform-specific parameters are also documented there.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e05e8539",
      "metadata": {
        "id": "e05e8539"
      },
      "outputs": [],
      "source": [
        "from modelconverter import convert\n",
        "\n",
        "labels = [\"robot\", \"tractor\", \"horse\", \"car\", \"person\", \"bear\"]\n",
        "\n",
        "converted_model = convert.RVC2(\n",
        "    api_key=HUBAI_API_KEY,\n",
        "    path=\"yolov8n_trained_datadreamer.pt\",\n",
        "    name=\"YOLOv8n DataDreamer\",\n",
        "    description_short=\"Detection model trained on a synthetic dataset generated with DataDreamer.\",\n",
        "    yolo_version=\"yolov8\",\n",
        "    yolo_input_shape=\"512 288\",\n",
        "    yolo_class_names=labels,\n",
        "    tasks=[\"OBJECT_DETECTION\"],\n",
        "    license_type=\"MIT\",\n",
        "    is_public=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a48fce",
      "metadata": {
        "id": "d3a48fce"
      },
      "source": [
        "We have successfully converted our trained model for an RVC2 device, so let's test it! Please copy the path to the downloaded archive with the converted model from the output log of the last code cell; we will use it in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1603940",
      "metadata": {
        "id": "e1603940"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"<YOUR_DOWNLOADED_MODEL_ARCHIVE_PATH>\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45da0825",
      "metadata": {
        "id": "45da0825"
      },
      "source": [
        "<a name=\"depthai-script\"></a>\n",
        "\n",
        "## üì∑ DepthAI Script"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03e2edae",
      "metadata": {
        "id": "03e2edae"
      },
      "source": [
        "To test our model on one of our cameras, we first need to install [`DepthAI`](https://rvc4.docs.luxonis.com/software/) in version 3 and [`DepthAI Nodes`](https://rvc4.docs.luxonis.com/software/ai-inference/depthai-nodes/). Moreover, the script we'll write must run locally and require a Luxonis device connected to your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5012f4e8",
      "metadata": {
        "id": "5012f4e8"
      },
      "outputs": [],
      "source": [
        "%pip install -q depthai==3.0.0rc2 -U\n",
        "%pip install -q depthai-nodes==0.3.0 -U"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c1952c",
      "metadata": {
        "id": "00c1952c"
      },
      "source": [
        "To run the model on a DepthAI device using the script below, please note the following:\n",
        "\n",
        "- You can view the output stream by opening [http://localhost:8082](http://localhost:8082) in your browser.\n",
        "\n",
        "- If you're running the script from a Jupyter Notebook, the output may not appear directly within the notebook. The script should print a link pointing to [http://localhost:8082](http://localhost:8082) for accessing the stream.\n",
        "\n",
        "- To stop the video stream, press **`q`** while focused on the visualizer page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40b8eb58",
      "metadata": {
        "id": "40b8eb58"
      },
      "outputs": [],
      "source": [
        "DEVICE = None # Set to None to use the default device, or you can specify a specific device IP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "809a0fea",
      "metadata": {
        "id": "809a0fea"
      },
      "outputs": [],
      "source": [
        "import depthai as dai\n",
        "from depthai_nodes.node import ParsingNeuralNetwork, ImgDetectionsBridge\n",
        "\n",
        "device = dai.Device(dai.DeviceInfo(DEVICE)) if DEVICE else dai.Device()\n",
        "platform = device.getPlatform()\n",
        "img_frame_type = dai.ImgFrame.Type.BGR888i if platform.name == \"RVC4\" else dai.ImgFrame.Type.BGR888p\n",
        "visualizer = dai.RemoteConnection(httpPort=8082)\n",
        "\n",
        "with dai.Pipeline(device) as pipeline:\n",
        "    cam = pipeline.create(dai.node.Camera).build()\n",
        "    nn_archive = dai.NNArchive(MODEL_PATH)\n",
        "    # Create the neural network node\n",
        "    nn_with_parser = pipeline.create(ParsingNeuralNetwork).build(\n",
        "        cam.requestOutput((512, 288), type=img_frame_type, fps=30),\n",
        "        nn_archive\n",
        "    )\n",
        "\n",
        "    # Bridge the detections to the visualizer\n",
        "    label_encoding = {k: v for k, v in enumerate(nn_archive.getConfig().model.heads[0].metadata.classes)}\n",
        "    bridge = pipeline.create(ImgDetectionsBridge).build(nn_with_parser.out)\n",
        "    bridge.setLabelEncoding(label_encoding)\n",
        "\n",
        "    # Configure the visualizer node\n",
        "    visualizer.addTopic(\"Video\", nn_with_parser.passthrough, \"images\")\n",
        "    visualizer.addTopic(\"Detections\", bridge.out, \"detections\")\n",
        "\n",
        "    pipeline.start()\n",
        "    visualizer.registerPipeline(pipeline)\n",
        "\n",
        "    while pipeline.isRunning():\n",
        "        key = visualizer.waitKey(1)\n",
        "        if key == ord(\"q\"):\n",
        "            print(\"Got q key from the remote connection!\")\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}